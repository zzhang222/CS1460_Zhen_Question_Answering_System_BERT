{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering\n",
    "from datasets import load_dataset\n",
    "from datasets import disable_caching\n",
    "disable_caching()\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# we set up some seeds so that we can reproduce results\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "def load_model(device):\n",
    "  tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
    "  model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
    "  model.to(device)\n",
    "  return model, tokenizer\n",
    "\n",
    "def get_start_end(inputs, answer):\n",
    "  m, n = len(inputs), len(answer)\n",
    "  for i in range(m - n + 1):\n",
    "    if len(answer) > 0 and all(inputs[i: i+n] == answer):\n",
    "      return i, i + n - 1\n",
    "  return 0, 0\n",
    "\n",
    "def preprocess(dataset, tokenizer):\n",
    "  input_ids, attention_masks, labels = [], [], []\n",
    "  for i in range(len(dataset)):\n",
    "    answer = tokenizer(dataset[i]['answers'][0]['span_text'], return_tensors = 'pt')['input_ids'].squeeze()[1:-1]\n",
    "    question = dataset[i]['questions'][0]['input_text']\n",
    "    context = dataset[i]['contexts']\n",
    "    encoding = tokenizer.encode_plus(\n",
    "      question,\n",
    "      context,\n",
    "      add_special_tokens      = True,\n",
    "      max_length              = tokenizer.model_max_length,\n",
    "      return_token_type_ids   = False,\n",
    "      return_attention_mask   = True,\n",
    "      return_tensors          = \"pt\",\n",
    "      padding                 = \"max_length\",\n",
    "      truncation              = True\n",
    "    )\n",
    "    input_id, attention_mask = encoding['input_ids'], encoding['attention_mask']\n",
    "    start, end = get_start_end(input_id.squeeze(), answer)\n",
    "    if start != 0 or end != 0:\n",
    "      labels.append((start, end))\n",
    "      input_ids.append(input_id)\n",
    "      attention_masks.append(attention_mask)\n",
    "  return input_ids, attention_masks, labels\n",
    "\n",
    "def preprocess_and_tokenize(dataset, tokenizer, batch_size):\n",
    "  input_ids, attention_masks, labels = preprocess(dataset, tokenizer)\n",
    "  dataset = QADataset(input_ids, attention_masks, labels)\n",
    "  return DataLoader(dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, input_ids, attention_masks, labels):\n",
    "\n",
    "    self.input_ids = input_ids\n",
    "    self.attention_masks = attention_masks\n",
    "    self.labels = labels\n",
    "\n",
    "  def __len__(self):\n",
    "\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return {\n",
    "      'input_ids': self.input_ids[index],\n",
    "      'attention_mask': self.attention_masks[index],\n",
    "      'labels': torch.tensor(self.labels[index], dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_loop(model, num_epochs, optimizer, lr_scheduler, train_data_loader, validation_data_loader, device):\n",
    "  train_losses, val_losses = [], []\n",
    "  for epoch in range(num_epochs):\n",
    "    # put the model in training mode (important that this is done each epoch,\n",
    "    # since we put the model into eval mode during validation)\n",
    "    model.train()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} training:\")\n",
    "    progress_bar = tqdm(range(len(train_data_loader)))\n",
    "    train_loss, val_loss, num_train, num_val = 0, 0, 0, 0\n",
    "\n",
    "    for i, batch in enumerate(train_data_loader):\n",
    "      start, end = batch['labels'][:, 0], batch['labels'][:, 1]\n",
    "      res = model(batch['input_ids'].to(device).squeeze(), batch['attention_mask'].to(device).squeeze(), start_positions = start.to(device), end_positions = end.to(device))\n",
    "      loss = res.loss\n",
    "      loss.backward()\n",
    "      batch_size = len(batch['input_ids'])\n",
    "      train_loss += loss.item() * batch_size\n",
    "      num_train += batch_size\n",
    "      optimizer.step()\n",
    "      lr_scheduler.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      progress_bar.update(1)\n",
    "    train_loss /= num_train\n",
    "\n",
    "    # print the epoch's average metrics\n",
    "    print(f\"Epoch {epoch+1} average training loss={train_loss}\")\n",
    "\n",
    "    # normally, validation would be more useful when training for many epochs\n",
    "    print(\"Running validation:\")\n",
    "    for i, batch in enumerate(validation_data_loader):\n",
    "      start, end = batch['labels'][:, 0], batch['labels'][:, 1]\n",
    "      res = model(batch['input_ids'].to(device).squeeze(), batch['attention_mask'].to(device).squeeze(), start_positions = start.to(device), end_positions = end.to(device))\n",
    "      loss = res.loss\n",
    "      batch_size = len(batch['input_ids'])\n",
    "      val_loss += loss.item() * batch_size\n",
    "      num_val += batch_size\n",
    "      progress_bar.update(1)\n",
    "    val_loss /= num_val\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} average validation loss={val_loss}\")\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "  return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def evaluate_metrics(input_ids, s, e, start, end):\n",
    "  total_matches, total_pred_length, total_ref_length = 0, 0, 0\n",
    "  for input_id, s1, e1, start1, end1 in zip(input_ids, s, e, start, end):\n",
    "    pred = input_id[s1: e1 + 1]\n",
    "    ref = input_id[start1: end1 + 1]\n",
    "    counter_pred = Counter(pred.detach().cpu().numpy())\n",
    "    counter_ref = Counter(ref.detach().cpu().numpy())\n",
    "    for word in counter_pred:\n",
    "      total_matches += min(counter_pred[word], counter_ref[word])\n",
    "    total_pred_length += len(pred)\n",
    "    total_ref_length += len(ref)\n",
    "  return total_matches, total_pred_length, total_ref_length\n",
    "\n",
    "def eval_loop(model, validation_data_loader, device):\n",
    "  progress_bar = tqdm(range(len(validation_data_loader)))\n",
    "  matches, pred_length, ref_length = 0, 0, 0\n",
    "  for i, batch in enumerate(validation_data_loader):\n",
    "    start, end = batch['labels'][:, 0].to(device), batch['labels'][:, 1].to(device)\n",
    "    res = model(batch['input_ids'].to(device).squeeze(), batch['attention_mask'].to(device).squeeze())\n",
    "    s, e = res.start_logits.argmax(axis = 1), res.end_logits.argmax(axis = 1)\n",
    "    total_matches, total_pred_length, total_ref_length = evaluate_metrics(batch['input_ids'].to(device).squeeze(), s, e, start, end)\n",
    "    matches += total_matches\n",
    "    pred_length += total_pred_length\n",
    "    ref_length += total_ref_length\n",
    "    progress_bar.update(1)\n",
    "  precision, recall =  matches / pred_length, matches / ref_length\n",
    "  f1 = 2/(1/precision + 1/recall)\n",
    "  return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-40f1f9282355cd35\n",
      "Found cached dataset json (/users/zzhang99/.cache/huggingface/datasets/json/default-40f1f9282355cd35/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b933ee4be04e83a42f7e2fd7b15e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-825013e737ca82fb\n",
      "Found cached dataset json (/users/zzhang99/.cache/huggingface/datasets/json/default-825013e737ca82fb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b32e42544e94ce1ab9647e85c8f3e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Preprocessing.\n",
      "Epoch 1 training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2547e0c48642d481d29470c41a5644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/578 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average training loss=1.5715172958002783\n",
      "Running validation:\n",
      "Epoch 1 average validation loss=1.4653942863054694\n",
      "Epoch 2 training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07b126113384c27bb44006e7bc33ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/578 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average training loss=0.8303669090398867\n",
      "Running validation:\n",
      "Epoch 2 average validation loss=1.5089201957469174\n",
      "Epoch 3 training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ea7392bfda4e7f9d9dc2ace26fbee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/578 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "def main():\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  batch_size = 24\n",
    "\n",
    "  model, tokenizer = load_model(device)\n",
    "  train = load_dataset('json', data_files = 'train.json')['train']\n",
    "  validation = load_dataset('json', data_files = 'dev.json')['train']\n",
    "\n",
    "  print('Start Preprocessing.')\n",
    "  train_data_loader = preprocess_and_tokenize(train, tokenizer, batch_size)\n",
    "  validation_data_loader = preprocess_and_tokenize(validation, tokenizer, batch_size)\n",
    "\n",
    "  num_epochs = 3\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-4)\n",
    "  lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=50,\n",
    "    num_training_steps=len(train_data_loader) * num_epochs\n",
    "  )\n",
    "\n",
    "  train_losses, val_losses = train_loop(model, num_epochs, optimizer, lr_scheduler, train_data_loader, validation_data_loader, device)\n",
    "  precision, recall, f1_score  = eval_loop(model, validation_data_loader, device)\n",
    "  \n",
    "  print(\"PRECISION: \", precision)\n",
    "  print(\"RECALL: \", recall)\n",
    "  print(\"F1-SCORE: \", f1_score)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
